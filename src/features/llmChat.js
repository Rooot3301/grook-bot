// src/features/llmChat.js
// RÃ©ponses conversationnelles via un LLM local (Ollama).
// Ce module permet au bot de rÃ©pondre de maniÃ¨re plus naturelle lorsque quelquâ€™un le mentionne.
// Il fonctionne sans API payanteÂ : il suffit dâ€™installer et de lancer Ollama en local.

// ParamÃ¨tres rÃ©cupÃ©rÃ©s depuis .env. Voir README pour les dÃ©tails d'installation.
const OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://127.0.0.1:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'llama3.1:8b';
const MAX_TOKENS = parseInt(process.env.LLM_MAX_TOKENS || '256', 10);
const TEMPERATURE = Number(process.env.LLM_TEMPERATURE || '0.4');
const SYSTEM_PROMPT = process.env.LLM_SYSTEM_PROMPT ||
  'Tu es Grook, un bot Discord utile et concis. RÃ©ponds toujours en franÃ§ais, avec humour quand câ€™est appropriÃ©.';
const ENABLED = String(process.env.LLM_ON_MENTION || 'true').toLowerCase() === 'true';

// MÃ©moire courte par utilisateur pour garder un minimum de contexte sur la conversation.
// Chaque entrÃ©eÂ : { role: 'user' | 'assistant', content: string, at: timestamp }
const memory = new Map();
const TTL_MS = 10 * 60 * 1000; // On garde 10 minutes dâ€™historique
const MAX_TURNS = 6; // On ne garde que les 6 derniers Ã©changes

function pushHistory(key, role, content) {
  const now = Date.now();
  const list = memory.get(key) || [];
  const next = list
    // Ne conserver que les messages rÃ©cents
    .filter((m) => now - m.at < TTL_MS)
    // Ajouter le nouveau message
    .concat([{ role, content: content.slice(0, 1200), at: now }])
    // Limiter le nombre de messages conservÃ©s
    .slice(-MAX_TURNS);
  memory.set(key, next);
  return next;
}

function buildPrompt(history) {
  const header = `SYSTEM:\n${SYSTEM_PROMPT}\n\n`;
  const turns = history
    .map((msg) => {
      const prefix = msg.role === 'user' ? 'USER' : 'ASSISTANT';
      return `${prefix}: ${msg.content}`;
    })
    .join('\n');
  return `${header}${turns}\nASSISTANT:`;
}

async function callOllama(prompt) {
  const body = {
    model: OLLAMA_MODEL,
    prompt,
    stream: false,
    options: {
      temperature: TEMPERATURE,
      num_predict: MAX_TOKENS,
    },
  };
  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), 20000); // 20Â s max
  try {
    const res = await fetch(`${OLLAMA_HOST}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body),
      signal: controller.signal,
    });
    if (!res.ok) throw new Error(`Ollama HTTP ${res.status}`);
    const data = await res.json();
    return (data.response || '').trim();
  } finally {
    clearTimeout(timeout);
  }
}

/**
 * Tente de gÃ©rer une mention avec le LLM. Retourne true si une rÃ©ponse a Ã©tÃ© envoyÃ©e.
 * @param {import('discord.js').Message} message
 * @param {import('discord.js').Client} client
 */
export async function handleMentionLLM(message, client) {
  if (!ENABLED) return false;
  // VÃ©rification basique pour ignorer certaines mentions
  if (!message.guild || !message.content) return false;
  if (!message.mentions || !message.mentions.users) return false;
  // Le bot est-il mentionnÃ©Â ?
  if (!message.mentions.users.has(client.user.id)) return false;
  // Ignorer les messages @everyone ou @here
  if (message.mentions.everyone) return false;
  // Ignorer si trop de mentions pour ne pas spammer
  if (message.mentions.users.size > 5) return false;
  // Extraire le contenu hors mention
  const cleaned = message.content
    .replace(new RegExp(`<@!?${client.user.id}>`, 'g'), '')
    .trim();
  if (!cleaned) return false;
  // Historique (contexte) par utilisateur/guilde
  const key = `${message.guild.id}:${message.author.id}`;
  const hist = pushHistory(key, 'user', cleaned);
  const prompt = buildPrompt(hist);
  try {
    // Indique Ã  Discord que le bot est en train de composer une rÃ©ponse
    await message.channel.sendTyping();
    const response = await callOllama(prompt);
    if (!response) return false;
    // Ajouter la rÃ©ponse dans lâ€™historique
    pushHistory(key, 'assistant', response);
    // Discord limite Ã  2000Â caractÃ¨res, on dÃ©coupe si besoin
    const chunks = response.match(/.{1,1800}/gs) || [response];
    for (const [i, chunk] of chunks.entries()) {
      if (i >= 3) break; // Ne pas envoyer plus de 3 messages dâ€™un coup
      await message.reply({ content: chunk });
    }
    return true;
  } catch (err) {
    console.error('[llmChat] erreurÂ :', err);
    try { await message.react('ğŸ’¤'); } catch {} // RÃ©action discrÃ¨te en cas d'Ã©chec
    return false;
  }
}